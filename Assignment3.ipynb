{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Priyesh Patel\n",
        "---\n",
        "CS 4395.001\n",
        "\n",
        "Assignment 3\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XLc7p_lVSaxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "WordNet is a lexical Database of semantic realationships between words. WordNet was a project started by a psychologist, George Miller, who was interested in how people hierarchically organize concepts. Within the WordNet nouns, verbs, adjectives, and adverbs are grouped into sets of congintive synonyms called synsets. Overall a WordNet can be seen as a combination of a dictionary and thesaurus."
      ],
      "metadata": {
        "id": "u0TfKkObELQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.book import text4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jkJ6FCxnHVis",
        "outputId": "50af3ea0-beed-4e55-9689-6430ee0c57a2"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get all sysnets for the selected noun 'life'\n",
        "wn.synsets('life', pos=wn.NOUN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qyLNGbqYHk61",
        "outputId": "18ea8c73-08c9-454a-d615-464a7c1b84d8"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('life.n.01'),\n",
              " Synset('life.n.02'),\n",
              " Synset('life.n.03'),\n",
              " Synset('animation.n.01'),\n",
              " Synset('life.n.05'),\n",
              " Synset('life.n.06'),\n",
              " Synset('life.n.07'),\n",
              " Synset('life.n.08'),\n",
              " Synset('liveliness.n.02'),\n",
              " Synset('life.n.10'),\n",
              " Synset('life.n.11'),\n",
              " Synset('biography.n.01'),\n",
              " Synset('life.n.13'),\n",
              " Synset('life_sentence.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected a synset\n",
        "noun_synset = wn.synset('life.n.01')\n",
        "\n",
        "# Extracting the definition, usage examples, and lemmas\n",
        "print('Definition: ', noun_synset.definition())\n",
        "print('Usage: ', noun_synset.examples())\n",
        "print('Lemmas: ', noun_synset.lemmas())\n",
        "\n",
        "print()\n",
        "# Closure Method to traverse from Karen Mazidi's Github (Chapter 7.2)\n",
        "hyper = lambda s: s.hypernyms()\n",
        "list(noun_synset.closure(hyper))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IId1Ww9PIR4C",
        "outputId": "45ecc279-2912-4cf6-d0df-6795365f20ed"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition:  a characteristic state or mode of living\n",
            "Usage:  ['social life', 'city life', 'real life']\n",
            "Lemmas:  [Lemma('life.n.01.life')]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('being.n.01'),\n",
              " Synset('state.n.02'),\n",
              " Synset('attribute.n.02'),\n",
              " Synset('abstraction.n.06'),\n",
              " Synset('entity.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nouns in WordNet are organized into hierarchies based on the hypernyms(higher)/hyponymy(lower) relation between synsets. With nouns we can see that entity is at the top of hierarchy for nouns. For the example of Life there were five more synset in its hierarchy one of which was 'begin' which also came up when we outputted the hypernyms for life."
      ],
      "metadata": {
        "id": "x0ig3jMXpF6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print hypernyms, hyponyms, meronyms,holonyms, antonym\n",
        "print('Hypernyms: ', noun_synset.hypernyms())\n",
        "print('Hyponyms: ', noun_synset.hyponyms())\n",
        "# Nouns might not have any meronyms, holonyms, or antonym\n",
        "print('Meronyms: ', noun_synset.member_meronyms())\n",
        "print('Holonyms: ', noun_synset.part_holonyms())\n",
        "print('Antonyms: ', noun_synset.lemmas()[0].antonyms())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "g8eknap_OZg1",
        "outputId": "0b34da03-8f90-442a-9755-7f15d7497f50"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms:  [Synset('being.n.01')]\n",
            "Hyponyms:  [Synset('ghetto.n.02')]\n",
            "Meronyms:  []\n",
            "Holonyms:  []\n",
            "Antonyms:  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get all sysnets for the selected verb 'drive'\n",
        "wn.synsets('drive', pos=wn.VERB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NoINOeMYUcsH",
        "outputId": "29b41221-37a8-45c3-b731-fca7da22a2d9"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('drive.v.01'),\n",
              " Synset('drive.v.02'),\n",
              " Synset('drive.v.03'),\n",
              " Synset('force.v.06'),\n",
              " Synset('drive.v.05'),\n",
              " Synset('repel.v.01'),\n",
              " Synset('drive.v.07'),\n",
              " Synset('drive.v.08'),\n",
              " Synset('drive.v.09'),\n",
              " Synset('tug.v.02'),\n",
              " Synset('drive.v.11'),\n",
              " Synset('drive.v.12'),\n",
              " Synset('drive.v.13'),\n",
              " Synset('drive.v.14'),\n",
              " Synset('drive.v.15'),\n",
              " Synset('drive.v.16'),\n",
              " Synset('drive.v.17'),\n",
              " Synset('drive.v.18'),\n",
              " Synset('drive.v.19'),\n",
              " Synset('drive.v.20'),\n",
              " Synset('drive.v.21'),\n",
              " Synset('drive.v.22')]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected a synset\n",
        "verb_synset = wn.synset('drive.v.01')\n",
        "\n",
        "# Extracting the definition, usage examples, and lemmas\n",
        "print('Definition: ', verb_synset.definition())\n",
        "print('Usage: ', verb_synset.examples())\n",
        "print('Lemmas: ', verb_synset.lemmas())\n",
        "\n",
        "print()\n",
        "# Closure Method to traverse from Karen Mazidi's Github (Chapter 7.2)\n",
        "hyper = lambda s: s.hypernyms()\n",
        "list(verb_synset.closure(hyper))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6wfTdWnbVtn5",
        "outputId": "698ee749-2178-490a-bf92-3246ff791dd3"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition:  operate or control a vehicle\n",
            "Usage:  ['drive a car or bus', 'Can you drive this four-wheel truck?']\n",
            "Lemmas:  [Lemma('drive.v.01.drive')]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('operate.v.03'),\n",
              " Synset('manipulate.v.02'),\n",
              " Synset('handle.v.04'),\n",
              " Synset('touch.v.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet also organizes verbs similar to nouns were they are organized into hierarchies based on hypernyms(higher)/hyponym(lower) relation between synsets. For example operate is a hypernym for drive. A difference we can see between noun and verb is that there isn't a common verb that is at the top of the verb hierarchy such as entity was for the noun hierarchy."
      ],
      "metadata": {
        "id": "TtAuDax0rA-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wn.morphy('drive'))\n",
        "print(wn.morphy('drive', wn.VERB))\n",
        "print(wn.morphy('drive', wn.NOUN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "87tMOgIYWKdX",
        "outputId": "265ab276-6bec-402d-e5c4-924184212081"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\n",
            "drive\n",
            "drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select two words that might be similar\n",
        "car = wn.synset('car.n.01')\n",
        "boat = wn.synset('boat.n.01')\n",
        "\n",
        "#Run Wu-Palmer Similarity Metric\n",
        "print('Wu-Palmer Similarity: ',wn.wup_similarity(car, boat))\n",
        "print()\n",
        "#Run the Lesk Algorithm on car\n",
        "car_sentence = ['He', 'drove', 'away', 'in', 'his', 'car','.']\n",
        "print('Lesk Algorithm on car: ', lesk(car_sentence, 'car'))\n",
        "print('Synset Definition: ', lesk(car_sentence, 'car').definition())\n",
        "print()\n",
        "#Run the Lesk Algorithm on boat\n",
        "boat_sentence = ['We', 'are', 'renting', 'a', 'boat', 'to', 'go', 'fishing', '.']\n",
        "print('Lesk Algorithm on boat: ', lesk(boat_sentence, 'boat'))\n",
        "print('Synset Definition: ', lesk(boat_sentence, 'boat').definition())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Rv9kODvlY_TL",
        "outputId": "124ea754-4b6c-4365-d7bb-dae2a64e051e"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wu-Palmer Similarity:  0.6956521739130435\n",
            "\n",
            "Lesk Algorithm on car:  Synset('car.n.04')\n",
            "Synset Definition:  where passengers ride up and down\n",
            "\n",
            "Lesk Algorithm on boat:  Synset('boat.v.01')\n",
            "Synset Definition:  ride in a boat on water\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the Wu-Palmer similarity metric it calculates how related two words are by considering the depth of their two synsets in the WordNet. After choosing car and boat for my words I expected them to fairly related which they were with a 70 percent similarity.\n",
        "\n",
        "As for the Lesk algorithm it returns the sysnet with the highest number of overlapping words between the definition and its sentence context in each synset and the target word. For the two examples I used 'car' and 'boat' the algorithm worked as expected for the sentence I provided for the boat and it return an accurate synset. However for the car example the synset that was return wasn't what I expected with the given sentence. I feel that it could have returned a close synset that would have relate closer to the particular sentence. "
      ],
      "metadata": {
        "id": "-N8kAmu6tK5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select an emotionally charged word\n",
        "wn.synsets('hate')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Q9z0kgzTfrBu",
        "outputId": "93cca7f6-2321-4b5a-bbf8-f2b8a997bcef"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('hate.n.01'), Synset('hate.v.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentiWordNet is a lexical resource for opinion mining build on top of WordNet. With sentiWordNet it is possible to analyze the sentiment such as positive, negative, or objective of some given text or sentence. Some possible use cases can be for customer sentiment or social media monitoring.\n",
        "\n",
        "After using SentiWordNet I was amazed at how accurate it was. For example 'hate' return higher value for it being negative and objective which is what you would exepect. It was also good at detecting sentences where they might be words that don't fall into a sentiment category such as 'I' and 'weather'. I think that SentiWordNet will come in handy for NLP application such as chatbots.\n"
      ],
      "metadata": {
        "id": "G1OJKKpf7gKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hate = swn.senti_synset('hate.n.01')\n",
        "print(hate)\n",
        "print(\"Positive score = \", hate.pos_score())\n",
        "print(\"Negative score = \", hate.neg_score())\n",
        "print(\"Objective score = \", hate.obj_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9mveTZ7hf89S",
        "outputId": "389b3061-0479-4fa4-fad8-8daa27ced221"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<hate.n.01: PosScore=0.125 NegScore=0.375>\n",
            "Positive score =  0.125\n",
            "Negative score =  0.375\n",
            "Objective score =  0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I hate cold weather'\n",
        "neg = 0\n",
        "pos = 0\n",
        "tokens = sentence.split() # split the sentence into tokens\n",
        "\n",
        "print('Polarity for each word: ',sentence)\n",
        "#iterate through each token and print polarity of each token \n",
        "for t in tokens:\n",
        "  syn_list = list(swn.senti_synsets(t))[0] \n",
        "  # take difference between the positive and negative score for polarity\n",
        "  polarity = syn_list.pos_score() - syn_list.neg_score() \n",
        "  print(t,\": \", polarity)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "K7bNdlO4hLIr",
        "outputId": "b94832b4-17c5-4c9e-d5ad-2c3a5ece2c81"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity for each word:  I hate cold weather\n",
            "I :  0.0\n",
            "hate :  -0.25\n",
            "cold :  -0.125\n",
            "weather :  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#output collocation for text4 (inaugral corpus)\n",
        "print(text4.collocations())\n",
        "text = ' '.join(text4.tokens).lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RvvfR_VGjmd5",
        "outputId": "9b21fbd0-1cd9-4aee-b98f-44269d936820"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate mutual information (P(x,y) / [P(x) * P(y)])\n",
        "import math\n",
        "collocation = 'indian tribes'\n",
        "coll_X = 'indian'\n",
        "coll_Y = 'tribes'\n",
        "\n",
        "# mutual information is the log of the probability\n",
        "vocab = len(set(text4))\n",
        "prob_indian_tribes = text.count('indian tribes')/vocab\n",
        "print(\"P(Indian Tribes) = \",prob_indian_tribes)\n",
        "prob_indian = text.count('indian')/vocab\n",
        "print(\"P(Indian) = \", prob_indian)\n",
        "prob_tribes = text.count('tribes')/vocab\n",
        "print('P(Tribes) = ', prob_tribes)\n",
        "pmi = math.log2(prob_indian_tribes / (prob_indian * prob_tribes))\n",
        "print('PMI = ', pmi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DGAgXtFQktEE",
        "outputId": "8a9a84ff-cc91-4b01-c2c4-0ecd9cdb11f5"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(Indian Tribes) =  0.000598503740648379\n",
            "P(Indian) =  0.0010972568578553616\n",
            "P(Tribes) =  0.000598503740648379\n",
            "PMI =  9.831882997592349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collocations are words that occur together that can give a greater meaning to the overall expression. Collocations can be found using point-wise mutual information(PMI), where if PMI = 0 - the two word are independent, if PMI > 0 - the two words may be a collocation, and if PMI < 0 the two words may not be a collocation.\n",
        "\n",
        "\n",
        "For my example I choose to use the collocation of Indian tribes. After calculating the mutual information between the two words we got a high PMI score which can indicate that these two words are likey collocations. In addition when looking at the probabilty of tribes in the text it is the same a the collocation of 'Indian tribes'. This shows that whenever tribes occurs in the text it is almost always preceded by the word indian."
      ],
      "metadata": {
        "id": "RerjEDPc7jNi"
      }
    }
  ]
}